"""
Base verifier class for RLVR.

This module defines the abstract interface that all verifiers must implement.
Verifiers are responsible for checking the correctness of model outputs
and providing verification signals for reward calculation.
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union, Tuple
from dataclasses import dataclass
from enum import Enum
import logging
from pathlib import Path
import json


class VerificationResult(Enum):
    """Enumeration of possible verification results."""
    CORRECT = "correct"
    INCORRECT = "incorrect"
    PARTIAL = "partial"
    ERROR = "error"
    TIMEOUT = "timeout"


@dataclass
class VerificationOutput:
    """Structured output from a verifier."""
    
    result: VerificationResult
    score: float  # Score between 0.0 and 1.0
    details: Dict[str, Any]  # Additional verification details
    error_message: Optional[str] = None
    execution_time: Optional[float] = None
    
    def __post_init__(self):
        """Validate the verification output."""
        if not 0.0 <= self.score <= 1.0:
            raise ValueError(f"Score must be between 0.0 and 1.0, got {self.score}")
        
        if self.result == VerificationResult.ERROR and not self.error_message:
            raise ValueError("Error message required when result is ERROR")


class BaseVerifier(ABC):
    """
    Abstract base class for all verifiers.
    
    Verifiers are responsible for checking the correctness of model outputs
    against ground truth or expected results. They provide structured feedback
    that can be used to compute rewards for reinforcement learning.
    
    Attributes:
        name (str): Name of the verifier
        description (str): Description of what the verifier does
        config (Dict[str, Any]): Configuration parameters
        logger (logging.Logger): Logger instance
    """
    
    def __init__(
        self,
        name: str,
        description: str = "",
        config: Optional[Dict[str, Any]] = None,
        logger: Optional[logging.Logger] = None
    ):
        """
        Initialize the verifier.
        
        Args:
            name: Name of the verifier
            description: Description of what the verifier does
            config: Configuration parameters
            logger: Logger instance
        """
        self.name = name
        self.description = description
        self.config = config or {}
        self.logger = logger or logging.getLogger(f"{__name__}.{name}")
        
        # Validate configuration
        self._validate_config()
    
    @abstractmethod
    def verify(
        self,
        instruction: str,
        model_output: str,
        expected_output: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> VerificationOutput:
        """
        Verify the correctness of a model output.
        
        Args:
            instruction: The instruction given to the model
            model_output: The output generated by the model
            expected_output: Expected output (if available)
            context: Additional context for verification
            
        Returns:
            VerificationOutput: Structured verification result
        """
        pass
    
    @abstractmethod
    def _validate_config(self) -> None:
        """
        Validate the verifier configuration.
        
        Raises:
            ValueError: If configuration is invalid
        """
        pass
    
    def batch_verify(
        self,
        instructions: List[str],
        model_outputs: List[str],
        expected_outputs: Optional[List[str]] = None,
        contexts: Optional[List[Dict[str, Any]]] = None
    ) -> List[VerificationOutput]:
        """
        Verify multiple model outputs in batch.
        
        Args:
            instructions: List of instructions
            model_outputs: List of model outputs
            expected_outputs: List of expected outputs (optional)
            contexts: List of additional contexts (optional)
            
        Returns:
            List of verification outputs
        """
        if len(instructions) != len(model_outputs):
            raise ValueError("Instructions and model_outputs must have the same length")
        
        if expected_outputs is not None and len(expected_outputs) != len(instructions):
            raise ValueError("Expected_outputs must have the same length as instructions")
        
        if contexts is not None and len(contexts) != len(instructions):
            raise ValueError("Contexts must have the same length as instructions")
        
        results = []
        for i, (instruction, model_output) in enumerate(zip(instructions, model_outputs)):
            expected = expected_outputs[i] if expected_outputs else None
            context = contexts[i] if contexts else None
            
            try:
                result = self.verify(instruction, model_output, expected, context)
                results.append(result)
            except Exception as e:
                self.logger.error(f"Verification failed for item {i}: {e}")
                results.append(VerificationOutput(
                    result=VerificationResult.ERROR,
                    score=0.0,
                    details={"error": str(e)},
                    error_message=str(e)
                ))
        
        return results
    
    def get_verification_stats(self, outputs: List[VerificationOutput]) -> Dict[str, Any]:
        """
        Compute statistics from verification outputs.
        
        Args:
            outputs: List of verification outputs
            
        Returns:
            Dictionary containing verification statistics
        """
        if not outputs:
            return {}
        
        total = len(outputs)
        correct = sum(1 for o in outputs if o.result == VerificationResult.CORRECT)
        incorrect = sum(1 for o in outputs if o.result == VerificationResult.INCORRECT)
        partial = sum(1 for o in outputs if o.result == VerificationResult.PARTIAL)
        errors = sum(1 for o in outputs if o.result == VerificationResult.ERROR)
        timeouts = sum(1 for o in outputs if o.result == VerificationResult.TIMEOUT)
        
        avg_score = sum(o.score for o in outputs) / total
        scores = [o.score for o in outputs]
        
        return {
            "total": total,
            "correct": correct,
            "incorrect": incorrect,
            "partial": partial,
            "errors": errors,
            "timeouts": timeouts,
            "accuracy": correct / total,
            "avg_score": avg_score,
            "min_score": min(scores),
            "max_score": max(scores),
            "std_score": (sum((s - avg_score) ** 2 for s in scores) / total) ** 0.5
        }
    
    def save_results(
        self,
        outputs: List[VerificationOutput],
        filepath: Union[str, Path]
    ) -> None:
        """
        Save verification results to a file.
        
        Args:
            outputs: List of verification outputs
            filepath: Path to save the results
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        # Convert outputs to serializable format
        serializable_outputs = []
        for output in outputs:
            serializable_outputs.append({
                "result": output.result.value,
                "score": output.score,
                "details": output.details,
                "error_message": output.error_message,
                "execution_time": output.execution_time
            })
        
        with open(filepath, 'w') as f:
            json.dump(serializable_outputs, f, indent=2)
        
        self.logger.info(f"Saved verification results to {filepath}")
    
    def load_results(self, filepath: Union[str, Path]) -> List[VerificationOutput]:
        """
        Load verification results from a file.
        
        Args:
            filepath: Path to load the results from
            
        Returns:
            List of verification outputs
        """
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"Results file not found: {filepath}")
        
        with open(filepath, 'r') as f:
            data = json.load(f)
        
        outputs = []
        for item in data:
            output = VerificationOutput(
                result=VerificationResult(item["result"]),
                score=item["score"],
                details=item["details"],
                error_message=item.get("error_message"),
                execution_time=item.get("execution_time")
            )
            outputs.append(output)
        
        return outputs
    
    def __str__(self) -> str:
        """String representation of the verifier."""
        return f"{self.__class__.__name__}(name='{self.name}')"
    
    def __repr__(self) -> str:
        """Detailed string representation of the verifier."""
        return f"{self.__class__.__name__}(name='{self.name}', config={self.config})" 