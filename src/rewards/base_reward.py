"""
Base reward class for RLVR.

This module defines the abstract interface that all reward functions must implement.
Reward functions are responsible for computing rewards based on verification outputs
and other training signals.
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union, Tuple
from dataclasses import dataclass
from enum import Enum
import logging
from pathlib import Path
import json
import numpy as np

from ..verifiers.base_verifier import VerificationOutput


class RewardType(Enum):
    """Enumeration of reward types."""
    VERIFICATION = "verification"
    HYBRID = "hybrid"
    CUSTOM = "custom"


@dataclass
class RewardOutput:
    """Structured output from a reward function."""
    
    reward: float  # The computed reward value
    reward_type: RewardType  # Type of reward
    components: Dict[str, float]  # Individual reward components
    metadata: Dict[str, Any]  # Additional metadata
    confidence: float  # Confidence in the reward (0.0 to 1.0)
    
    def __post_init__(self):
        """Validate the reward output."""
        if not isinstance(self.reward, (int, float)):
            raise ValueError(f"Reward must be a number, got {type(self.reward)}")
        
        if not 0.0 <= self.confidence <= 1.0:
            raise ValueError(f"Confidence must be between 0.0 and 1.0, got {self.confidence}")
        
        if not isinstance(self.components, dict):
            raise ValueError("Components must be a dictionary")


class BaseReward(ABC):
    """
    Abstract base class for all reward functions.
    
    Reward functions are responsible for computing rewards based on verification
    outputs and other training signals. They provide structured feedback that
    guides the reinforcement learning process.
    
    Attributes:
        name (str): Name of the reward function
        description (str): Description of what the reward function does
        config (Dict[str, Any]): Configuration parameters
        logger (logging.Logger): Logger instance
        reward_type (RewardType): Type of reward function
    """
    
    def __init__(
        self,
        name: str,
        description: str = "",
        config: Optional[Dict[str, Any]] = None,
        logger: Optional[logging.Logger] = None,
        reward_type: RewardType = RewardType.CUSTOM
    ):
        """
        Initialize the reward function.
        
        Args:
            name: Name of the reward function
            description: Description of what the reward function does
            config: Configuration parameters
            logger: Logger instance
            reward_type: Type of reward function
        """
        self.name = name
        self.description = description
        self.config = config or {}
        self.logger = logger or logging.getLogger(f"{__name__}.{name}")
        self.reward_type = reward_type
        
        # Validate configuration
        self._validate_config()
    
    @abstractmethod
    def compute_reward(
        self,
        instruction: str,
        model_output: str,
        verification_outputs: List[VerificationOutput],
        context: Optional[Dict[str, Any]] = None
    ) -> RewardOutput:
        """
        Compute the reward for a model output.
        
        Args:
            instruction: The instruction given to the model
            model_output: The output generated by the model
            verification_outputs: List of verification outputs
            context: Additional context for reward computation
            
        Returns:
            RewardOutput: Structured reward result
        """
        pass
    
    @abstractmethod
    def _validate_config(self) -> None:
        """
        Validate the reward function configuration.
        
        Raises:
            ValueError: If configuration is invalid
        """
        pass
    
    def batch_compute_rewards(
        self,
        instructions: List[str],
        model_outputs: List[str],
        verification_outputs_list: List[List[VerificationOutput]],
        contexts: Optional[List[Dict[str, Any]]] = None
    ) -> List[RewardOutput]:
        """
        Compute rewards for multiple model outputs in batch.
        
        Args:
            instructions: List of instructions
            model_outputs: List of model outputs
            verification_outputs_list: List of verification output lists
            contexts: List of additional contexts (optional)
            
        Returns:
            List of reward outputs
        """
        if len(instructions) != len(model_outputs):
            raise ValueError("Instructions and model_outputs must have the same length")
        
        if len(verification_outputs_list) != len(instructions):
            raise ValueError("Verification outputs must have the same length as instructions")
        
        if contexts is not None and len(contexts) != len(instructions):
            raise ValueError("Contexts must have the same length as instructions")
        
        rewards = []
        for i, (instruction, model_output, verification_outputs) in enumerate(
            zip(instructions, model_outputs, verification_outputs_list)
        ):
            context = contexts[i] if contexts else None
            
            try:
                reward = self.compute_reward(instruction, model_output, verification_outputs, context)
                rewards.append(reward)
            except Exception as e:
                self.logger.error(f"Reward computation failed for item {i}: {e}")
                rewards.append(RewardOutput(
                    reward=0.0,
                    reward_type=self.reward_type,
                    components={"error": 0.0},
                    metadata={"error": str(e)},
                    confidence=0.0
                ))
        
        return rewards
    
    def get_reward_stats(self, rewards: List[RewardOutput]) -> Dict[str, Any]:
        """
        Compute statistics from reward outputs.
        
        Args:
            rewards: List of reward outputs
            
        Returns:
            Dictionary containing reward statistics
        """
        if not rewards:
            return {}
        
        reward_values = [r.reward for r in rewards]
        confidence_values = [r.confidence for r in rewards]
        
        # Component statistics
        component_stats = {}
        all_components = set()
        for reward in rewards:
            all_components.update(reward.components.keys())
        
        for component in all_components:
            component_values = [r.components.get(component, 0.0) for r in rewards]
            component_stats[component] = {
                "mean": np.mean(component_values),
                "std": np.std(component_values),
                "min": np.min(component_values),
                "max": np.max(component_values)
            }
        
        return {
            "total": len(rewards),
            "reward_mean": np.mean(reward_values),
            "reward_std": np.std(reward_values),
            "reward_min": np.min(reward_values),
            "reward_max": np.max(reward_values),
            "confidence_mean": np.mean(confidence_values),
            "confidence_std": np.std(confidence_values),
            "component_stats": component_stats
        }
    
    def save_rewards(
        self,
        rewards: List[RewardOutput],
        filepath: Union[str, Path]
    ) -> None:
        """
        Save reward outputs to a file.
        
        Args:
            rewards: List of reward outputs
            filepath: Path to save the rewards
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        # Convert rewards to serializable format
        serializable_rewards = []
        for reward in rewards:
            serializable_rewards.append({
                "reward": reward.reward,
                "reward_type": reward.reward_type.value,
                "components": reward.components,
                "metadata": reward.metadata,
                "confidence": reward.confidence
            })
        
        with open(filepath, 'w') as f:
            json.dump(serializable_rewards, f, indent=2)
        
        self.logger.info(f"Saved rewards to {filepath}")
    
    def load_rewards(self, filepath: Union[str, Path]) -> List[RewardOutput]:
        """
        Load reward outputs from a file.
        
        Args:
            filepath: Path to load the rewards from
            
        Returns:
            List of reward outputs
        """
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"Rewards file not found: {filepath}")
        
        with open(filepath, 'r') as f:
            data = json.load(f)
        
        rewards = []
        for item in data:
            reward = RewardOutput(
                reward=item["reward"],
                reward_type=RewardType(item["reward_type"]),
                components=item["components"],
                metadata=item["metadata"],
                confidence=item["confidence"]
            )
            rewards.append(reward)
        
        return rewards
    
    def normalize_reward(self, reward: float, min_val: float = -1.0, max_val: float = 1.0) -> float:
        """
        Normalize a reward value to a specified range.
        
        Args:
            reward: The reward value to normalize
            min_val: Minimum value of the normalized range
            max_val: Maximum value of the normalized range
            
        Returns:
            Normalized reward value
        """
        # Default normalization to [-1, 1] range
        # This can be overridden by subclasses for different normalization schemes
        return np.clip(reward, min_val, max_val)
    
    def apply_reward_shaping(self, reward: float, context: Optional[Dict[str, Any]] = None) -> float:
        """
        Apply reward shaping to improve learning.
        
        Args:
            reward: The base reward value
            context: Additional context for shaping
            
        Returns:
            Shaped reward value
        """
        # Default implementation - no shaping
        # This can be overridden by subclasses for specific shaping strategies
        return reward
    
    def compute_confidence(
        self,
        verification_outputs: List[VerificationOutput],
        context: Optional[Dict[str, Any]] = None
    ) -> float:
        """
        Compute confidence in the reward based on verification outputs.
        
        Args:
            verification_outputs: List of verification outputs
            context: Additional context
            
        Returns:
            Confidence value between 0.0 and 1.0
        """
        if not verification_outputs:
            return 0.0
        
        # Default confidence computation based on verification results
        total_verifications = len(verification_outputs)
        successful_verifications = sum(
            1 for v in verification_outputs 
            if v.result.value in ["correct", "partial"]
        )
        
        return successful_verifications / total_verifications
    
    def __str__(self) -> str:
        """String representation of the reward function."""
        return f"{self.__class__.__name__}(name='{self.name}')"
    
    def __repr__(self) -> str:
        """Detailed string representation of the reward function."""
        return f"{self.__class__.__name__}(name='{self.name}', config={self.config})" 