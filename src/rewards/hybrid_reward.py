"""
Hybrid reward function for RLVR.

This module implements a hybrid reward function that combines multiple
verification signals and reward components to provide comprehensive
feedback for reinforcement learning.
"""

import time
import traceback
from typing import Dict, Any, List, Optional, Union, Tuple
import logging
import numpy as np
from collections import defaultdict

from .base_reward import BaseReward, RewardOutput, RewardType
from ..verifiers.base_verifier import VerificationOutput, VerificationResult


class HybridReward(BaseReward):
    """
    Hybrid reward function that combines multiple verification signals.
    
    This reward function can combine different types of verification outputs
    and apply various weighting schemes to compute comprehensive rewards.
    """
    
    def __init__(
        self,
        name: str = "hybrid_reward",
        config: Optional[Dict[str, Any]] = None,
        logger: Optional[logging.Logger] = None
    ):
        """
        Initialize the hybrid reward function.
        
        Args:
            name: Name of the reward function
            config: Configuration parameters
            logger: Logger instance
        """
        default_config = {
            # Component weights
            "verification_weight": 0.6,  # Weight for verification-based rewards
            "quality_weight": 0.2,  # Weight for output quality metrics
            "diversity_weight": 0.1,  # Weight for diversity/repetition penalties
            "efficiency_weight": 0.1,  # Weight for efficiency metrics
            
            # Verification scoring
            "correct_score": 1.0,  # Score for correct verification
            "partial_score": 0.7,  # Score for partial verification
            "incorrect_score": 0.0,  # Score for incorrect verification
            "error_score": -0.1,  # Score for verification errors
            
            # Quality metrics
            "length_penalty": 0.1,  # Penalty for very short/long outputs
            "repetition_penalty": 0.2,  # Penalty for repetitive content
            "coherence_bonus": 0.1,  # Bonus for coherent reasoning
            
            # Diversity metrics
            "vocabulary_diversity_weight": 0.05,  # Weight for vocabulary diversity
            "structure_diversity_weight": 0.05,  # Weight for structural diversity
            
            # Efficiency metrics
            "response_time_weight": 0.05,  # Weight for response time
            "computation_efficiency_weight": 0.05,  # Weight for computation efficiency
            
            # Normalization
            "normalize_rewards": True,  # Whether to normalize rewards
            "reward_range": (-1.0, 1.0),  # Range for normalized rewards
            
            # Confidence computation
            "confidence_threshold": 0.5,  # Threshold for confidence computation
            "uncertainty_penalty": 0.1,  # Penalty for uncertain verifications
        }
        
        if config:
            default_config.update(config)
        
        super().__init__(
            name=name,
            description="Combines multiple verification signals for comprehensive rewards",
            config=default_config,
            logger=logger,
            reward_type=RewardType.HYBRID
        )
    
    def _validate_config(self) -> None:
        """Validate the reward function configuration."""
        weights = [
            self.config["verification_weight"],
            self.config["quality_weight"],
            self.config["diversity_weight"],
            self.config["efficiency_weight"]
        ]
        
        if not np.isclose(sum(weights), 1.0, atol=1e-6):
            raise ValueError("Component weights must sum to 1.0")
        
        if not all(0.0 <= w <= 1.0 for w in weights):
            raise ValueError("All weights must be between 0.0 and 1.0")
        
        if not 0.0 <= self.config["confidence_threshold"] <= 1.0:
            raise ValueError("Confidence threshold must be between 0.0 and 1.0")
    
    def compute_reward(
        self,
        instruction: str,
        model_output: str,
        verification_outputs: List[VerificationOutput],
        context: Optional[Dict[str, Any]] = None
    ) -> RewardOutput:
        """
        Compute the hybrid reward for a model output.
        
        Args:
            instruction: The instruction given to the model
            model_output: The output generated by the model
            verification_outputs: List of verification outputs
            context: Additional context for reward computation
            
        Returns:
            RewardOutput: Structured reward result
        """
        start_time = time.time()
        
        try:
            # Compute individual reward components
            components = {}
            
            # 1. Verification-based reward
            verification_reward = self._compute_verification_reward(verification_outputs)
            components["verification"] = verification_reward
            
            # 2. Quality-based reward
            quality_reward = self._compute_quality_reward(instruction, model_output, context)
            components["quality"] = quality_reward
            
            # 3. Diversity-based reward
            diversity_reward = self._compute_diversity_reward(model_output, context)
            components["diversity"] = diversity_reward
            
            # 4. Efficiency-based reward
            efficiency_reward = self._compute_efficiency_reward(model_output, context)
            components["efficiency"] = efficiency_reward
            
            # Combine rewards using weights
            total_reward = (
                self.config["verification_weight"] * verification_reward +
                self.config["quality_weight"] * quality_reward +
                self.config["diversity_weight"] * diversity_reward +
                self.config["efficiency_weight"] * efficiency_reward
            )
            
            # Apply reward shaping
            shaped_reward = self.apply_reward_shaping(total_reward, context)
            
            # Normalize reward if configured
            if self.config["normalize_rewards"]:
                shaped_reward = self.normalize_reward(
                    shaped_reward,
                    self.config["reward_range"][0],
                    self.config["reward_range"][1]
                )
            
            # Compute confidence
            confidence = self.compute_confidence(verification_outputs, context)
            
            # Apply uncertainty penalty
            if confidence < self.config["confidence_threshold"]:
                shaped_reward -= self.config["uncertainty_penalty"]
            
            computation_time = time.time() - start_time
            
            return RewardOutput(
                reward=shaped_reward,
                reward_type=self.reward_type,
                components=components,
                metadata={
                    "computation_time": computation_time,
                    "verification_count": len(verification_outputs),
                    "output_length": len(model_output),
                    "confidence": confidence
                },
                confidence=confidence
            )
            
        except Exception as e:
            computation_time = time.time() - start_time
            self.logger.error(f"Hybrid reward computation failed: {e}")
            return RewardOutput(
                reward=0.0,
                reward_type=self.reward_type,
                components={"error": 0.0},
                metadata={"error": str(e), "computation_time": computation_time},
                confidence=0.0
            )
    
    def _compute_verification_reward(self, verification_outputs: List[VerificationOutput]) -> float:
        """
        Compute reward based on verification outputs.
        
        Args:
            verification_outputs: List of verification outputs
            
        Returns:
            Verification-based reward
        """
        if not verification_outputs:
            return 0.0
        
        total_score = 0.0
        total_weight = 0.0
        
        for verification in verification_outputs:
            # Determine score based on verification result
            if verification.result == VerificationResult.CORRECT:
                score = self.config["correct_score"]
            elif verification.result == VerificationResult.PARTIAL:
                score = self.config["partial_score"]
            elif verification.result == VerificationResult.INCORRECT:
                score = self.config["incorrect_score"]
            elif verification.result == VerificationResult.ERROR:
                score = self.config["error_score"]
            else:
                score = 0.0
            
            # Use verification score as weight if available
            weight = verification.score if verification.score > 0 else 1.0
            total_score += score * weight
            total_weight += weight
        
        return total_score / total_weight if total_weight > 0 else 0.0
    
    def _compute_quality_reward(self, instruction: str, model_output: str, context: Optional[Dict[str, Any]] = None) -> float:
        """
        Compute reward based on output quality metrics.
        
        Args:
            instruction: The instruction given to the model
            model_output: The output generated by the model
            context: Additional context
            
        Returns:
            Quality-based reward
        """
        quality_score = 0.0
        
        # Length penalty
        output_length = len(model_output)
        instruction_length = len(instruction)
        
        # Penalize very short or very long outputs
        if output_length < instruction_length * 0.5:
            quality_score -= self.config["length_penalty"]
        elif output_length > instruction_length * 5.0:
            quality_score -= self.config["length_penalty"]
        
        # Repetition penalty
        repetition_score = self._compute_repetition_penalty(model_output)
        quality_score -= repetition_score * self.config["repetition_penalty"]
        
        # Coherence bonus
        coherence_score = self._compute_coherence_score(model_output)
        quality_score += coherence_score * self.config["coherence_bonus"]
        
        return np.clip(quality_score, -1.0, 1.0)
    
    def _compute_diversity_reward(self, model_output: str, context: Optional[Dict[str, Any]] = None) -> float:
        """
        Compute reward based on diversity metrics.
        
        Args:
            model_output: The output generated by the model
            context: Additional context
            
        Returns:
            Diversity-based reward
        """
        diversity_score = 0.0
        
        # Vocabulary diversity
        vocab_diversity = self._compute_vocabulary_diversity(model_output)
        diversity_score += vocab_diversity * self.config["vocabulary_diversity_weight"]
        
        # Structural diversity
        structure_diversity = self._compute_structure_diversity(model_output)
        diversity_score += structure_diversity * self.config["structure_diversity_weight"]
        
        return np.clip(diversity_score, -1.0, 1.0)
    
    def _compute_efficiency_reward(self, model_output: str, context: Optional[Dict[str, Any]] = None) -> float:
        """
        Compute reward based on efficiency metrics.
        
        Args:
            model_output: The output generated by the model
            context: Additional context
            
        Returns:
            Efficiency-based reward
        """
        efficiency_score = 0.0
        
        # Response time (if available in context)
        if context and "response_time" in context:
            response_time = context["response_time"]
            # Normalize response time (shorter is better)
            if response_time < 1.0:  # Very fast
                efficiency_score += 0.1
            elif response_time > 10.0:  # Very slow
                efficiency_score -= 0.1
        
        # Computation efficiency (if available in context)
        if context and "computation_cost" in context:
            computation_cost = context["computation_cost"]
            # Normalize computation cost (lower is better)
            if computation_cost < 1000:  # Low cost
                efficiency_score += 0.1
            elif computation_cost > 10000:  # High cost
                efficiency_score -= 0.1
        
        return np.clip(efficiency_score, -1.0, 1.0)
    
    def _compute_repetition_penalty(self, text: str) -> float:
        """Compute penalty for repetitive content."""
        words = text.lower().split()
        if len(words) < 2:
            return 0.0
        
        # Count word repetitions
        word_counts = defaultdict(int)
        for word in words:
            word_counts[word] += 1
        
        # Calculate repetition ratio
        total_words = len(words)
        unique_words = len(word_counts)
        repetition_ratio = 1.0 - (unique_words / total_words)
        
        return repetition_ratio
    
    def _compute_coherence_score(self, text: str) -> float:
        """Compute coherence score for the text."""
        # Simple coherence indicators
        coherence_indicators = [
            "therefore", "thus", "hence", "so", "because", "since",
            "however", "but", "although", "while", "if", "then"
        ]
        
        text_lower = text.lower()
        indicator_count = sum(1 for indicator in coherence_indicators if indicator in text_lower)
        
        # Normalize by text length
        text_length = len(text.split())
        if text_length == 0:
            return 0.0
        
        return min(1.0, indicator_count / (text_length * 0.1))
    
    def _compute_vocabulary_diversity(self, text: str) -> float:
        """Compute vocabulary diversity score."""
        words = text.lower().split()
        if len(words) < 2:
            return 0.0
        
        unique_words = len(set(words))
        total_words = len(words)
        
        # Type-token ratio
        return unique_words / total_words
    
    def _compute_structure_diversity(self, text: str) -> float:
        """Compute structural diversity score."""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.0
        
        # Count different sentence structures
        structures = []
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # Simple structure classification
            if sentence.startswith(('The', 'A', 'An', 'This', 'That')):
                structures.append('declarative')
            elif sentence.endswith('?'):
                structures.append('interrogative')
            elif sentence.endswith('!'):
                structures.append('exclamatory')
            else:
                structures.append('other')
        
        unique_structures = len(set(structures))
        total_structures = len(structures)
        
        return unique_structures / total_structures if total_structures > 0 else 0.0
    
    def apply_reward_shaping(self, reward: float, context: Optional[Dict[str, Any]] = None) -> float:
        """
        Apply reward shaping to improve learning.
        
        Args:
            reward: The base reward value
            context: Additional context for shaping
            
        Returns:
            Shaped reward value
        """
        shaped_reward = reward
        
        # Apply context-specific shaping
        if context:
            # Bonus for following instructions closely
            if context.get("instruction_following", False):
                shaped_reward += 0.1
            
            # Penalty for safety violations
            if context.get("safety_violation", False):
                shaped_reward -= 0.2
            
            # Bonus for helpfulness
            if context.get("helpful", False):
                shaped_reward += 0.05
        
        return np.clip(shaped_reward, -1.0, 1.0)
    
    def compute_confidence(
        self,
        verification_outputs: List[VerificationOutput],
        context: Optional[Dict[str, Any]] = None
    ) -> float:
        """
        Compute confidence in the reward based on verification outputs.
        
        Args:
            verification_outputs: List of verification outputs
            context: Additional context
            
        Returns:
            Confidence value between 0.0 and 1.0
        """
        if not verification_outputs:
            return 0.0
        
        # Base confidence from verification results
        total_verifications = len(verification_outputs)
        successful_verifications = sum(
            1 for v in verification_outputs 
            if v.result in [VerificationResult.CORRECT, VerificationResult.PARTIAL]
        )
        
        base_confidence = successful_verifications / total_verifications
        
        # Adjust confidence based on verification details
        confidence_adjustments = []
        for verification in verification_outputs:
            if verification.execution_time:
                # Shorter execution times indicate more reliable verification
                if verification.execution_time < 1.0:
                    confidence_adjustments.append(0.1)
                elif verification.execution_time > 10.0:
                    confidence_adjustments.append(-0.1)
            
            if verification.score > 0.8:
                confidence_adjustments.append(0.05)
            elif verification.score < 0.3:
                confidence_adjustments.append(-0.05)
        
        # Apply adjustments
        adjusted_confidence = base_confidence + np.mean(confidence_adjustments) if confidence_adjustments else base_confidence
        
        return np.clip(adjusted_confidence, 0.0, 1.0) 